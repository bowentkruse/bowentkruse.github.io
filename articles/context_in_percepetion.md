---
layout: post
title: Context & Perception with Computer Vision
---

*September 2024*

<p>&nbsp;</p>

When my family or friends ask what I do, I say that computer vision automates visual perception. This of course is recieved with the same amount of confusion as any other explanation but the point is valid and is a solid reminder that like many fields related to AI/Machine Learning, computer vision is rooted in lessons learned from our brains. 

<p>&nbsp;</p>

### Context in Human Perception
Psychologists would be mad at me if I weighed in on the debate between the distinction between [cognition vs. perception](https://www.psychologicalscience.org/observer/cognition-and-perception-is-there-really-a-distinction) or even just [top-down vs. bottom-up perception](https://jackwestin.com/resources/mcat-content/perception/bottom-up-and-top-down-processing#:~:text=%E2%80%A2-,Perception%20refers%20to%20the%20way%20sensory%20information%20is%20organized%2C%20interpreted,(top%2Ddown%20processing).). However everyone can agree that stuff you already know helps you percieve new stuff correctly.

<p>&nbsp;</p>

### Context in Computer Vision
As mentioned a few hundred pixels above, computer vision engineers automate visual tasks: automating infrastructure inspections, automating the process of reading license plates, etc. While customers may ask for 99.9% accuracy, modern computer vision models ***when deployed*** never quite reach that standard. In production, 